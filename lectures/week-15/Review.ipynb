{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "- Course Evaluations\n",
    "\n",
    "- [Final Project Submission Form](https://forms.gle/b9tffJTQZQBJaJWMA)\n",
    "\n",
    "- [Github Repo Link](https://github.com/mguner/UMBC_DATA602)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-1\n",
    "\n",
    "- AI > ML > DL \n",
    "\n",
    "- Their interaction with DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__What is ML?__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Old and vague definition: Field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "- A CP `learn` from experience $E$ w.r.t. some task $T$ and some performance measure $P$, if its performance on $T$ improves with experience $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Week-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Traditional Programming vs ML__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Traditional Programming: Programmer writes a code and computer process the input according to this pre-defined set of rules.\n",
    "\n",
    "- Computer takes inputs and outputs and produce a program (set of rules or relations) that can be used to process future inputs to produce (predict) outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Types of ML__\n",
    "\n",
    "- Supervised\n",
    "\n",
    "- Unsupervised\n",
    "\n",
    "- Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Components of ML Systems__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Define the problem to be solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Collect data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Choose an algorithm class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Choose an optimization metric (Objective function, cost function) for the learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. Choose a metric for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-3: Supervised Learning Problem\n",
    "\n",
    "__Linear Regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Model: $Y = \\beta_0 + \\beta_1 \\mathbf{X_1} + \\cdots + \\beta_{p} \\mathbf{X_{p}} + \\mathbf{\\mathcal{E}}$ \n",
    "\n",
    "- Objective Function, Cost Function: Least Squeares Approach\n",
    "\n",
    "- Evaluation Metrics: $R^{2}$: Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Using sklearn for Linear Regression__\n",
    "\n",
    "- Import\n",
    "\n",
    "- Instantiate \n",
    "\n",
    "- fit\n",
    "\n",
    "- Predict \n",
    "\n",
    "Framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Checking Assumptions__\n",
    "\n",
    "- Residual Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-4: Supervised Learning\n",
    "\n",
    "__Classification Problems__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Introduction: \n",
    "\n",
    "    - Different Classification problem examples\n",
    "    \n",
    "    - Comparison of Linear Regression and Classification problems\n",
    "\n",
    "- Binary vs Multi-Class Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Algorithm: Logistic Regression__\n",
    "\n",
    "- Model: Log of odds and use of logit function\n",
    "\n",
    "- Objective-Cost Function: Negative loglikelihood\n",
    "\n",
    "- Evaluation: Confusion Matrix with python and accuracy,recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Logistic Regression with Sklearn__\n",
    "\n",
    "- Probabilities as predictions (`predict_proba` method)\n",
    "\n",
    "- Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-5: Model Selection and Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Training error vs Test Error\n",
    "\n",
    "- Bias-Variance Trade Off\n",
    "\n",
    "- Different cross validation approaches for estimating the test error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Regularization Techniques__\n",
    "\n",
    "- Ridge and Lasso (L2 and L1 penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-6: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Regression trees\n",
    "\n",
    "- Classification Trees\n",
    "\n",
    "    - Entropy, Gini Impurity and Information Gain\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Decision Trees with Sklearn__\n",
    "\n",
    "- Important parameters: `max_depth`, `min_samples_split`, `max_features`\n",
    "\n",
    "- Plotting tree structure with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-7: Review\n",
    "\n",
    "- Preprocessing: StandardScaler, Normalizer\n",
    "\n",
    "- Preprocessing: PolynomialFeatures\n",
    "\n",
    "- Preprocessing: OHE - LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Model Selection: Train-Test split\n",
    "\n",
    "- Model Selection: Cross_validate\n",
    "\n",
    "- Model Selection: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest__\n",
    "\n",
    "- Advantage of ensemble methods\n",
    "\n",
    "- Bootstrapping\n",
    "\n",
    "- Bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-8: Unsupervised Learning\n",
    "\n",
    "__Clustering__\n",
    "\n",
    "- Introduction to Unsupervised Learning Problems\n",
    "\n",
    "- Introduction to Clustering Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Algorithm: KMeans\n",
    "\n",
    "- Objective Function: Inertia\n",
    "\n",
    "- Evaluation Metrics: Silhoutte and Calinski-Harabasz metrics, elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-9: Unsupervised Learning II\n",
    "\n",
    "__Dimension Reduction__\n",
    "\n",
    "- Introduction to the problem of dimension reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Algorithm: PCA\n",
    "- Objective Function: Explained Variance\n",
    "- Elbow Method and Explained Variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-10: Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- A Brief history of Neural Networks\n",
    "\n",
    "- Perceptron Architecture \n",
    "\n",
    "- Multilayer Perceptron Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-11: Training Deep Neural Networks\n",
    "\n",
    "[Colab Notebook](https://colab.research.google.com/drive/1q6n3ChKX0xrMKsJ_HO3YmqAC6L6Gm4N2?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Activation Functions:\n",
    "\n",
    "    * Relu, tanh, sigmoid\n",
    "\n",
    "- Forward Propagation\n",
    "\n",
    "- Back Propagation\n",
    "\n",
    "- Optimizers\n",
    "\n",
    "    - SGD, MiniBatch SGD, Momentum, RMSProp, Adam   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-12: Convolutional Neural Networks\n",
    "\n",
    "- Working with image data\n",
    "\n",
    "- Filters\n",
    "\n",
    "    - Convolution operations\n",
    "\n",
    "    - Strides and padding\n",
    "    \n",
    "     - Pooling Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-13: Convolutional Neural Networks with Keras\n",
    "\n",
    "[Colab Notebook](https://colab.research.google.com/drive/171WSgSJRAbk93AyspLvrj3HpPZkeMuK2?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Data Preparation\n",
    "\n",
    "- Subsampling Data\n",
    "\n",
    "- CNN with Keras\n",
    "\n",
    "    - Conv2d layer\n",
    "    \n",
    "    - MaxPooling2D layer\n",
    "    \n",
    "- Preprocessing with Keras\n",
    "\n",
    "    - ImageDataGenerator\n",
    "    \n",
    "- Saving and Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-14: CNN - Transfer Learning\n",
    "\n",
    "[Colab Notebook](https://colab.research.google.com/drive/1ahikLmTH6LVe1jJBtQ8mrtlJeW7zynov?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Transfer Learning\n",
    "\n",
    "- Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week-15: Closing Remarks"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
