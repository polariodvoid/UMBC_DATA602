# Week-9 Feature Extraction

This week we will cover dimension reduction techniques and feature extraction techniques. 

______

Chapter-10 of Introduction to Statistical Learning p373 - p385

Here are some notes for your reading.

__10.1 The Challenge of Unsupervised Learning__

We already covered this section but you might want to review this part.

__10.2 Principal Component Analysis__

- What does PCA do?

- What are the Principal component directions?

- What are the Principal Components?

- Why is the pair-wise scatter plot approach is not practical?

- PCA provides a tool for visualization by finding a low-dimensional representation of a data set that contains as much as ... (fill in the blank)

- If you can try to understand equations 10.1, 10.2, 10.3 but if you are having difficulty to make sense of them just skip them in your first reading.

- What is a good geometric interpretation for the first principal component?

- Understand why should we scale the data before PCA?

- What is explained variance and how is it related with the principal components?

- How do we decide how many principal components to use?

- After you read the material you might want to reproduce the result that has been shared in this textbook with Pyhon. You can find the dataset in the class repo. 

- Finally if time allows you can check: Python Machine Learning (3rd Edition), chapter-5 Compressing Data via Dimensionality Reduction.
